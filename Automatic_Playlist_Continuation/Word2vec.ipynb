{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import FastText\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = pd.read_csv('../Datasets/Copy of explicit_data - Playlist_data.csv')\n",
    "user_playlist_data = pd.read_csv('../Datasets/Copy of explicit_data - User_playlists.csv')\n",
    "songs = pd.read_csv('../Datasets/Copy of explicit_data - Songs - All.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_playlist_data.columns = ['playlist_id','timestamp','order','song_id']\n",
    "# df_merge = pd.merge(user_playlist_data, songs.drop_duplicates(['song_id']), on=\"song_id\", how=\"left\")\n",
    "# df_merge['ratings'] = np.ones((69,), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlists with particular set of songs using a word2vec model\n",
    "\n",
    "def get_playlist_name(playlist_id, df_merge):\n",
    "    playlist_name = df_merge.loc[df_merge['playlist_id']==playlist_id]\n",
    "    return playlist_name\n",
    "\n",
    "def get_bag_of_words(playlist_id, df_merge):\n",
    "    playlist_name = get_playlist_name(playlist_id, df_merge)\n",
    "    songs_names = playlist_name.Title.values\n",
    "    song_name_clean = [re.sub(r'[^\\w]', ' ', str(item))for item in songs_names]\n",
    "    song_name_clean = [re.sub(r\" \\d+\", '', str(item.strip())) for item in song_name_clean]\n",
    "        \n",
    "    sentences = list()\n",
    "    bof = []\n",
    "    for item in song_name_clean:\n",
    "        sentences.append(item.split())\n",
    "    unique_sentence = np.unique(sentences)\n",
    "        \n",
    "    for i in range (len(unique_sentence)):\n",
    "        for j in range (len(unique_sentence[i])):\n",
    "            bof.append(unique_sentence[i][j])\n",
    "    return unique_sentence, sentences, bof\n",
    "\n",
    "def word2vec_model_build(playlist_id, df_merge):\n",
    "    unique_sentence, sentences, bof = get_bag_of_words(playlist_id, df_merge)\n",
    "    model = Word2Vec(workers=1, \\\n",
    "            size=50, min_count = 1, \\\n",
    "            window = 3, sample = 1e-3, sg = 1)\n",
    "    model.build_vocab(sentences = unique_sentence)\n",
    "    model.train(sentences = unique_sentence,  total_examples=len(sentences), epochs=10)\n",
    "    model.init_sims(replace=True)\n",
    "    return model\n",
    "\n",
    "def avg_sentence_vector(song_id, playlist_id, df_merge, num_features = 50):\n",
    "    #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    song = songs['Title'].loc[songs['song_id'] == song_id]\n",
    "    for i in song:\n",
    "        words = i.split()\n",
    "    unique_sentence, sentences, bof = get_bag_of_words(playlist_id, df_merge)\n",
    "    model = word2vec_model_build(playlist_id, df_merge)\n",
    "    for word in words:\n",
    "        if word in bof:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if nwords>0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def calculate_cosine_similarity_pairwise(song_id, playlist_id, df_merge):\n",
    "    playlist_name = get_playlist_name(playlist_id, df_merge)\n",
    "    first_trac_id = playlist_name.song_id.values[0]\n",
    "    first_track_vector = avg_sentence_vector(first_trac_id, playlist_id, df_merge)\n",
    "    suggestion_vectors = avg_sentence_vector(song_id, playlist_id, df_merge)\n",
    "    \n",
    "    sen1_sen2_similarity =  cosine_similarity(first_track_vector.reshape(1, -1),suggestion_vectors.reshape(1, -1))\n",
    "    \n",
    "    return sen1_sen2_similarity\n",
    "\n",
    "def make_suggestion(songs, playlist_id, df_merge):\n",
    "    song_list = songs.song_id.values\n",
    "    suggestions = []\n",
    "    for idx in song_list:\n",
    "        similarity_score = calculate_cosine_similarity_pairwise(idx, playlist_id, df_merge)\n",
    "        for value in similarity_score:\n",
    "            if value>0:\n",
    "                suggestions.append(idx)\n",
    "            else:\n",
    "                continue\n",
    "    return suggestions\n",
    "\n",
    "# find for similar playlists and find item similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 67, 248]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_suggestion(songs, 103, df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
